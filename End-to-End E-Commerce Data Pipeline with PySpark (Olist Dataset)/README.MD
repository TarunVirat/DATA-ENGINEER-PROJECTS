# ğŸ“¦ End-to-End E-Commerce Data Pipeline with PySpark (Olist Dataset)

This project implements a full **end-to-end data pipeline** on the [Olist Brazilian E-Commerce Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce), covering **data ingestion, cleaning, transformation, integration, optimization, and serving**. The goal is to demonstrate how a real-world retail dataset can be processed using **PySpark** and prepared for advanced analytics such as customer retention, seller performance, and payment trends.

## ğŸ“Š Dataset Overview
The dataset contains **100k+ orders** made at Olist Store, an e-commerce platform in Brazil. It links **customers, sellers, products, payments, reviews, and geolocation** into a relational schema.

![Dataset Schema](13285472-59db-43ae-95a8-f00762563128.png)

**Main tables:**
- `olist_orders_dataset` â†’ orders metadata (status, timestamps).  
- `olist_order_items_dataset` â†’ product items per order.  
- `olist_order_payments_dataset` â†’ payment method & value.  
- `olist_order_reviews_dataset` â†’ customer reviews & scores.  
- `olist_products_dataset` â†’ product catalog.  
- `olist_sellers_dataset` â†’ sellersâ€™ info.  
- `olist_customers_dataset` â†’ customer info.  
- `olist_geolocation_dataset` â†’ geolocation (lat/lon by ZIP).  

## âš™ï¸ Project Workflow
This repo is structured into **5 modules**, each represented by a Jupyter Notebook (`.ipynb`).

### Module 1 â€“ Data Ingestion & Exploration
- Ingest raw CSVs into **HDFS/Spark DataFrames**.  
- Validate schema and datatypes.  
- Initial exploratory analysis (row counts, missing values, distributions).  

### Module 2 â€“ Data Cleaning & Transformation
- Handle nulls, duplicates, and inconsistent records.  
- Convert columns (e.g., timestamps to proper `timestamp`).  
- Apply business logic transformations (e.g., order status normalization).  

### Module 3 â€“ Data Integration & Aggregation
- Join across multiple datasets (orders â†” items â†” payments â†” reviews).  
- Aggregate metrics:  
  - First & last purchase dates per customer.  
  - Average Order Value (AOV).  
  - Seller performance by review score.  

### Module 4 â€“ Property Optimization
- Optimize storage with **Parquet** format.  
- Apply **partitioning/bucketing** for faster queries.  
- Cache frequently used tables.  

### Module 5 â€“ Data Serving
- Store final curated datasets back into HDFS (`/processed`).  
- Make datasets query-ready for BI tools and ML pipelines.  
- Example queries:  
  - Customer retention analysis.  
  - Weekend vs weekday purchase behavior.  
  - Payment trends by installment count.  

## ğŸ“‚ Repository Structure

## ğŸ› ï¸ Tech Stack
- **Apache Spark (PySpark)** â€“ distributed ETL & analytics  
- **HDFS (Hadoop Distributed File System)** â€“ storage  
- **Jupyter Notebooks** â€“ development & visualization  
- **Parquet** â€“ optimized columnar storage  
-

## ğŸ‘¨â€ğŸ’» Author
**Tarun Peela**  
ğŸ“ Tampa, FL  
ğŸ“ 813-613-8906  
ğŸ“§ tarunpeela0@gmail.com  
 

## ğŸš€ How to Run
1. Clone this repo & download the [Kaggle dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce).  
2. Upload the CSVs to HDFS path `/user/<username>/olist/`.  
3. Launch Jupyter Notebook inside your Spark cluster (GCP, AWS EMR, or local).  
4. Run notebooks in order (`Module 1 â†’ Module 5`).  
5. Processed Parquet files will be available under `/user/<username>/olist/processed/`.

## ğŸ“ˆ Example Analysis
- Customer Retention (first vs last order per unique customer).  
- Seller Performance by review score & revenue.  
- Payment method popularity & installment analysis.  
- Geographical spread of orders across Brazil.  

## ğŸ“Œ References
- [Kaggle â€“ Olist Brazilian E-Commerce Dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)  
- Olist Official Documentation  
